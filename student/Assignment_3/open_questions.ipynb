{
  "cells": [
    {
      "cell_type": "raw",
      "id": "c7fcefa2",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    toc-title: Contents\n",
        "    toc-depth: 4\n",
        "    self-contained: true\n",
        "    number-sections: false\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Models for Sentiment Classification\n",
        "\n",
        "**Author:** Junho Hong  \n",
        "\n",
        "This notebook contains analysis and reflection on six neural models trained for sentiment classification on the Financial PhraseBank dataset:\n",
        "- MLP (mean-pooled FastText)\n",
        "- RNN (SentenceTransformer sequences)\n",
        "- LSTM (padded FastText sequences)\n",
        "- GRU (SentenceTransformer sequences)\n",
        "- BERT (pretrained transformer, fine-tuned)\n",
        "- GPT-2 (pretrained transformer, fine-tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Results\n",
        "\n",
        "| Rank | Model | Test Accuracy | Test Macro F1 | Architecture Type |\n",
        "|------|-------|--------------|---------------|-------------------|\n",
        "| 1 | **BERT** | **0.8253** | **0.8090** | Pretrained Transformer (Encoder) |\n",
        "| 2 | **GPT-2** | 0.8061 | 0.7931 | Pretrained Transformer (Decoder) |\n",
        "| 3 | **GRU** | 0.7785 | 0.7520 | Sequential (SentenceTransformer) |\n",
        "| 4 | **LSTM** | 0.7648 | 0.7375 | Sequential (FastText) |\n",
        "| 5 | **RNN** | 0.7180 | 0.7023 | Sequential (SentenceTransformer) |\n",
        "| 6 | **MLP** | 0.7001 | 0.6699 | Feed-forward (mean pooling) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Training Dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 MLP Training Curves\n",
        "\n",
        "![MLP Training Curves](outputs/mlp_training_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 LSTM Training Curves\n",
        "\n",
        "![LSTM Training Curves](outputs/lstm_training_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Analysis: Overfitting vs Underfitting\n",
        "\n",
        "#### MLP Model:\n",
        "The MLP shows **slight overfitting** characteristics:\n",
        "- Training accuracy reaches ~75% while validation plateaus at ~72-73%\n",
        "- Training F1 reaches ~0.71 while validation plateaus at ~0.70\n",
        "- Training loss continues decreasing to ~0.58 while validation loss stabilizes at ~0.60\n",
        "- The gap between train/val metrics is relatively small (~2-3%), indicating mild overfitting\n",
        "\n",
        "**Evidence:** The training curves show steady improvement on the training set while validation metrics plateau after epoch 20, creating a small but consistent gap.\n",
        "\n",
        "#### LSTM Model:\n",
        "The LSTM shows **significant overfitting**:\n",
        "- Training accuracy reaches ~93% while validation plateaus at ~77%\n",
        "- Training F1 reaches ~0.92 while validation plateaus at ~0.75\n",
        "- Training loss drops to ~0.17 while validation loss increases to ~0.88\n",
        "- Large gap (~16% accuracy, ~0.17 F1) between train and validation performance\n",
        "\n",
        "#### Why LSTM overfits more than MLP:\n",
        "1. **Model Capacity**: LSTM has more parameters (~280K) than MLP (~39K)\n",
        "2. **Sequential Information**: LSTM can memorize specific word sequences from training data\n",
        "3. **Complexity**: Two-layer LSTM with recurrent connections has higher capacity to fit training data patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Architectural/Training Changes to Address Overfitting\n",
        "\n",
        "#### For LSTM:\n",
        "1. **Stronger Regularization**:\n",
        "   - Increase dropout from 0.3 to 0.5\n",
        "   - Add recurrent dropout in LSTM layers\n",
        "   - Apply weight decay (L2 regularization)\n",
        "2. **Reduce Model Capacity**:\n",
        "   - Use 1 LSTM layer instead of 2\n",
        "   - Reduce hidden dimension from 128 to 64\n",
        "3. **Gradient Clipping**: Prevent exploding gradients that can cause overfitting\n",
        "4. **Learning Rate Scheduling**: More aggressive learning rate reduction when validation performance plateaus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Effect of Class Weights on Training\n",
        "\n",
        "Class weights were computed based on the imbalanced distribution:\n",
        "- Negative: 604 samples (weight ≈ 2.67)\n",
        "- Neutral: 2879 samples (weight ≈ 0.56)\n",
        "- Positive: 1363 samples (weight ≈ 1.18)\n",
        "\n",
        "#### Impact on Training Stability:\n",
        "1. **Loss Weighting**: Negative class errors contribute ~5x more to loss than Neutral class errors\n",
        "2. **Learning Focus**: Model learns to pay more attention to minority classes (Negative, Positive)\n",
        "3. **Macro F1 Improvement**: Without class weights, model would overpredict Neutral class; with weights, performance is more balanced across classes\n",
        "\n",
        "#### Impact on Final Performance:\n",
        "- Class weights helped achieve better **Macro F1** scores (which treat all classes equally)\n",
        "- Without weights, the model would likely achieve higher overall accuracy but lower F1 on minority classes\n",
        "- The weighted loss made training more stable by preventing the model from simply learning to predict the majority class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Performance and Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 MLP Confusion Matrix\n",
        "\n",
        "![MLP Confusion Matrix](outputs/mlp_confusion_matrix.png)\n",
        "\n",
        "**Test Performance:**\n",
        "- Accuracy: 70.01%\n",
        "- Macro F1: 0.6699"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 LSTM Confusion Matrix\n",
        "\n",
        "![LSTM Confusion Matrix](outputs/lstm_confusion_matrix.png)\n",
        "\n",
        "**Test Performance:**\n",
        "- Accuracy: 76.48%\n",
        "- Macro F1: 0.7375"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Generalization Comparison\n",
        "\n",
        "**LSTM generalizes better to the test set** based on multiple metrics:\n",
        "\n",
        "| Metric | MLP | LSTM | Difference |\n",
        "|--------|-----|------|------------|\n",
        "| Test Accuracy | 70.01% | 76.48% | **+6.47%** |\n",
        "| Test Macro F1 | 0.6699 | 0.7375 | **+0.0676** |\n",
        "\n",
        "#### Evidence from Confusion Matrices:\n",
        "\n",
        "**MLP Errors:**\n",
        "- Negative class: 72/91 correct (79.1% recall)\n",
        "- Neutral class: 300/432 correct (69.4% recall)\n",
        "- Positive class: 137/204 correct (67.2% recall)\n",
        "- High confusion between Neutral→Positive (98 errors)\n",
        "\n",
        "**LSTM Errors:**\n",
        "- Negative class: 75/91 correct (82.4% recall)\n",
        "- Neutral class: 350/432 correct (81.0% recall)\n",
        "- Positive class: 131/204 correct (64.2% recall)\n",
        "- Better at distinguishing Neutral class (81% vs 69%)\n",
        "\n",
        "#### Why LSTM Generalizes Better:\n",
        "1. **Sequential Context**: LSTM captures word order and dependencies that MLP's mean pooling loses\n",
        "2. **Contextual Understanding**: LSTM processes sequences token-by-token, preserving syntactic structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Most Frequently Misclassified Class\n",
        "\n",
        "#### Analysis from Confusion Matrices:\n",
        "\n",
        "**MLP:**\n",
        "- Negative: 19 errors out of 91 (20.9% error rate)\n",
        "- Neutral: 132 errors out of 432 (30.6% error rate)\n",
        "- Positive: **67 errors out of 204 (32.8% error rate)** ← **Highest**\n",
        "\n",
        "**LSTM:**\n",
        "- Negative: 16 errors out of 91 (17.6% error rate)\n",
        "- Neutral: 82 errors out of 432 (19.0% error rate)\n",
        "- Positive: **73 errors out of 204 (35.8% error rate)** ← **Highest**\n",
        "\n",
        "**Answer: The Positive class is most frequently misclassified in both models.**\n",
        "\n",
        "#### Reasons for Positive Class Misclassification:\n",
        "\n",
        "1. **Semantic Ambiguity in Financial Text**:\n",
        "   - Financial positive sentiment is often understated (e.g., \"modest gains,\" \"slight improvement\")\n",
        "   - These phrases lack strong positive indicators, making them hard to distinguish from Neutral\n",
        "   - Example: \"The company reported stable earnings\" could be Neutral or mildly Positive\n",
        "\n",
        "2. **Model Limitations**:\n",
        "   - Both models struggle to capture nuanced sentiment in professional financial writing\n",
        "   - Mean pooling (MLP) loses word order that might indicate positive framing\n",
        "   - LSTM's sequential processing doesn't fully capture subtle financial sentiment cues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cross-Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Additional Model Confusion Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### RNN\n",
        "![RNN Confusion Matrix](outputs/rnn_confusion_matrix.png)\n",
        "**Performance:** Accuracy = 71.80%, Macro F1 = 0.7023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GRU\n",
        "![GRU Confusion Matrix](outputs/gru_confusion_matrix.png)\n",
        "**Performance:** Accuracy = 77.85%, Macro F1 = 0.7520"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BERT\n",
        "![BERT Confusion Matrix](outputs/bert_confusion_matrix.png)\n",
        "**Performance:** Accuracy = 82.53%, Macro F1 = 0.8090"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GPT-2\n",
        "![GPT Confusion Matrix](outputs/gpt_confusion_matrix.png)\n",
        "**Performance:** Accuracy = 80.61%, Macro F1 = 0.7931"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Training Curves Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### RNN Training Curves\n",
        "![RNN Learning Curves](outputs/rnn_f1_learning_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GRU Training Curves\n",
        "![GRU Learning Curves](outputs/gru_f1_learning_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BERT Training Curves\n",
        "![BERT Learning Curves](outputs/bert_f1_learning_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GPT-2 Training Curves\n",
        "![GPT Learning Curves](outputs/gpt_f1_learning_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 How Mean-Pooled FastText Embeddings Limited MLP Performance\n",
        "\n",
        "Mean pooling creates a **bag-of-words representation** that loses information:\n",
        "\n",
        "#### What is Lost:\n",
        "1. **Word Order**: \"profits increased significantly\" vs \"significantly increased profits\" → same vector\n",
        "2. **Negation**: \"not good\" vs \"good\" → vectors are averaged, weakening negation signal\n",
        "3. **Syntactic Structure**: Cannot capture subject-verb-object relationships\n",
        "4. **Context-Dependent Meaning**: \"the bank reported losses\" (financial) vs \"the river bank\" → word \"bank\" has same embedding\n",
        "\n",
        "#### Impact on Sentiment Classification:\n",
        "- **Example 1**: \"The company did not meet expectations\" vs \"The company met expectations\"\n",
        "  - Mean pooling produces very similar vectors despite opposite sentiments\n",
        "  - MLP cannot distinguish these because negation word \"not\" is averaged in\n",
        "\n",
        "- **Example 2**: \"Sales increased while profits decreased\" \n",
        "  - Sequential models see the contrast structure\n",
        "  - MLP only sees average of \"increased\" and \"decreased\" embeddings\n",
        "\n",
        "#### Quantitative Evidence:\n",
        "- MLP F1: 0.6699 (lowest among all models)\n",
        "- Gap to LSTM: +0.0676 F1 (10% relative improvement)\n",
        "- Gap to BERT: +0.1391 F1 (21% relative improvement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 LSTM's Sequential Processing Advantage Over MLP\n",
        "\n",
        "**1. Preserves Word Order:**\n",
        "- LSTM processes tokens sequentially: word₁ → word₂ → word₃\n",
        "- Can learn that \"not profitable\" is different from \"profitable not\"\n",
        "\n",
        "**2. Captures Long-Range Dependencies:**\n",
        "- LSTM's memory cells maintain information across the sequence\n",
        "- Can connect subject at start of sentence to verb at end\n",
        "\n",
        "**3. Handles Negation Better:**\n",
        "- Sequential processing allows LSTM to learn negation patterns\n",
        "- \"not good\" is processed as: \"not\" updates hidden state → \"good\" is modulated by that state\n",
        "\n",
        "**4. Contextual Word Representation:**\n",
        "- Same word in different positions/contexts produces different hidden states\n",
        "- \"bank\" in \"financial bank\" vs \"river bank\" gets different representations based on preceding words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Did Fine-Tuned LLMs Outperform Classical Baselines? Ranking and Key Factors\n",
        "\n",
        "**Answer: Yes, significantly.** BERT (F1 0.8090) and GPT-2 (0.7931) outperform the best classical baseline, GRU (0.7520), by about 5–8% relative.\n",
        "\n",
        "#### Ranking by Test Macro F1\n",
        "\n",
        "| Rank | Model | F1 | Architecture | Key differentiator |\n",
        "|------|-------|-----|--------------|--------------------|\n",
        "| 1 | BERT | 0.8090 | Pretrained Transformer | Bidirectional, contextual embeddings, self-attention |\n",
        "| 2 | GPT-2 | 0.7931 | Pretrained Transformer | Unidirectional decoder, contextual embeddings |\n",
        "| 3 | GRU | 0.7520 | Gated RNN | Sequential + gating; less overfitting than LSTM on small data |\n",
        "| 4 | LSTM | 0.7375 | Gated RNN | Sequential + memory; more overfitting than GRU here |\n",
        "| 5 | RNN | 0.7023 | Vanilla RNN | No gating, weaker long-range dependencies |\n",
        "| 6 | MLP | 0.6699 | Feed-forward | Mean pooling loses word order and negation |\n",
        "\n",
        "#### Why LLMs Lead\n",
        "\n",
        "1. **Pretraining & transfer** — BERT/GPT are pretrained on huge corpora; fine-tuning adapts that knowledge to financial sentiment. Classical models learn from scratch on ~4.8k samples.\n",
        "2. **Contextual embeddings** — BERT/GPT give context-dependent representations; classical models use static embeddings (e.g. FastText/SentenceTransformer).\n",
        "3. **Self-attention** — Transformers let every token attend to every other token; RNNs are sequential and can dilute long-range information.\n",
        "4. **Bidirectionality (BERT)** — BERT sees full context (left and right); RNNs and GPT are unidirectional.\n",
        "\n",
        "#### Why This Ranking Within Tiers\n",
        "\n",
        "- **BERT > GPT-2:** Bidirectional context and encoder design suit classification better than a decoder-only model.\n",
        "- **GRU > LSTM:** On this small dataset, GRU’s simpler gating generalizes better (LSTM showed stronger overfitting in training curves).\n",
        "- **RNN below gated RNNs:** No gating and vanishing gradients limit long-range dependency modeling.\n",
        "- **MLP last:** Mean pooling loses word order and negation, so bag-of-words is too weak for sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI Use Disclosure\n",
        "\n",
        "In completing this assignment, I used Claude for:\n",
        "- Debugging PyTorch compatibility issues when writing scripts\n",
        "- Structuring code with proper documentation and comments\n",
        "- Organizing the structure of this reflection notebook\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
