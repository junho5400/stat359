{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30ce988",
   "metadata": {},
   "source": [
    "## Word Analogy Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a661e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/junhohong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load PyTorch embeddings\n",
    "with open('word2vec_embeddings.pkl', 'rb') as f:\n",
    "    pytorch_data = pickle.load(f)\n",
    "    pytorch_embeddings = pytorch_data['embeddings']\n",
    "    pytorch_word2idx = pytorch_data['word2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30183b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "king - man + woman = ? (Expected: queen)\n",
      "  ✓ 1. queen           (0.582)\n",
      "    2. prince          (0.582)\n",
      "    3. daughter        (0.571)\n",
      "    4. throne          (0.542)\n",
      "    5. heir            (0.538)\n",
      "\n",
      "brother - man + woman = ? (Expected: sister)\n",
      "    1. daughter        (0.723)\n",
      "    2. mother          (0.647)\n",
      "    3. wife            (0.625)\n",
      "  ✓ 4. sister          (0.624)\n",
      "    5. married         (0.614)\n",
      "\n",
      "uncle - man + woman = ? (Expected: aunt)\n",
      "  ✓ 1. aunt            (0.629)\n",
      "    2. grandfather     (0.621)\n",
      "    3. mother          (0.609)\n",
      "    4. grandmother     (0.600)\n",
      "    5. eldest          (0.597)\n",
      "\n",
      "boy - man + woman = ? (Expected: girl)\n",
      "    1. daddy           (0.450)\n",
      "    2. girlfriend      (0.439)\n",
      "  ✓ 3. girl            (0.433)\n",
      "    4. teenagers       (0.431)\n",
      "    5. partner         (0.409)\n",
      "\n",
      "paris - france + italy = ? (Expected: rome)\n",
      "    1. genoa           (0.523)\n",
      "    2. turin           (0.498)\n",
      "    3. villa           (0.487)\n",
      "    4. vienna          (0.473)\n",
      "    5. ttingen         (0.469)\n",
      "\n",
      "london - england + france = ? (Expected: paris)\n",
      "  ✓ 1. paris           (0.480)\n",
      "    2. en              (0.467)\n",
      "    3. seven           (0.455)\n",
      "    4. casablanca      (0.453)\n",
      "    5. frankfurt       (0.441)\n",
      "\n",
      "tokyo - japan + china = ? (Expected: beijing)\n",
      "    1. shanghai        (0.545)\n",
      "    2. hong            (0.543)\n",
      "    3. guangzhou       (0.538)\n",
      "    4. chang           (0.538)\n",
      "    5. guangdong       (0.517)\n",
      "  (Found at rank 6)\n",
      "\n",
      "good - better + bad = ? (Expected: worse)\n",
      "    1. ghost           (0.515)\n",
      "    2. something       (0.514)\n",
      "    3. evil            (0.508)\n",
      "    4. remember        (0.489)\n",
      "    5. angry           (0.480)\n",
      "\n",
      "big - bigger + small = ? (Expected: smaller)\n",
      "    1. searching       (0.409)\n",
      "    2. tai             (0.401)\n",
      "    3. rican           (0.368)\n",
      "    4. tan             (0.365)\n",
      "    5. che             (0.362)\n",
      "\n",
      "walk - walked + talk = ? (Expected: talked)\n",
      "    1. actors          (0.477)\n",
      "    2. hackers         (0.451)\n",
      "    3. tv              (0.427)\n",
      "    4. radio           (0.424)\n",
      "    5. wiki            (0.414)\n",
      "\n",
      "go - went + do = ? (Expected: did)\n",
      "    1. happen          (0.545)\n",
      "    2. how             (0.544)\n",
      "    3. want            (0.543)\n",
      "    4. will            (0.534)\n",
      "    5. exist           (0.533)\n",
      "\n",
      "dog - dogs + cat = ? (Expected: cats)\n",
      "    1. terrier         (0.470)\n",
      "    2. ass             (0.462)\n",
      "    3. billed          (0.457)\n",
      "    4. doo             (0.445)\n",
      "    5. girl            (0.443)\n",
      "\n",
      "man - men + woman = ? (Expected: women)\n",
      "    1. girl            (0.560)\n",
      "    2. baby            (0.525)\n",
      "    3. herself         (0.517)\n",
      "    4. blonde          (0.500)\n",
      "    5. my              (0.489)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Accuracy: 6/13 (46.2%)\n",
      "Missing vocabulary: 0\n",
      "\n",
      "Category        Accuracy\n",
      "------------------------------\n",
      "Comparative     0/2 (0%)\n",
      "Gender          4/4 (100%)\n",
      "Geography       2/3 (67%)\n",
      "Plural          0/2 (0%)\n",
      "Verb Tense      0/2 (0%)\n"
     ]
    }
   ],
   "source": [
    "def word_analogy(word_a, word_b, word_c, embeddings, word2idx, top_n=10):\n",
    "    \"\"\"\n",
    "    Solve word analogy: word_a - word_b + word_c = ?\n",
    "    \n",
    "    Returns:\n",
    "        list of (word, similarity) tuples, or None if words missing\n",
    "    \"\"\"\n",
    "    # Check if all words exist\n",
    "    if word_a not in word2idx or word_b not in word2idx or word_c not in word2idx:\n",
    "        missing = []\n",
    "        if word_a not in word2idx: missing.append(word_a)\n",
    "        if word_b not in word2idx: missing.append(word_b)\n",
    "        if word_c not in word2idx: missing.append(word_c)\n",
    "        return None, missing\n",
    "    \n",
    "    # Get embeddings\n",
    "    vec_a = embeddings[word2idx[word_a]]\n",
    "    vec_b = embeddings[word2idx[word_b]]\n",
    "    vec_c = embeddings[word2idx[word_c]]\n",
    "    \n",
    "    # Compute target vector: a - b + c\n",
    "    target_vec = vec_a - vec_b + vec_c\n",
    "    \n",
    "    # Compute cosine similarity with all words\n",
    "    similarities = []\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        vec = embeddings[idx]\n",
    "        sim = np.dot(target_vec, vec) / (np.linalg.norm(target_vec) * np.linalg.norm(vec))\n",
    "        similarities.append((word, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n], None\n",
    "\n",
    "# Define analogy test cases\n",
    "analogies = [\n",
    "    # Gender relationships\n",
    "    (\"king\", \"man\", \"woman\", \"queen\", \"Gender\"),\n",
    "    (\"brother\", \"man\", \"woman\", \"sister\", \"Gender\"),\n",
    "    (\"uncle\", \"man\", \"woman\", \"aunt\", \"Gender\"),\n",
    "    (\"boy\", \"man\", \"woman\", \"girl\", \"Gender\"),\n",
    "    \n",
    "    # Geography\n",
    "    (\"paris\", \"france\", \"italy\", \"rome\", \"Geography\"),\n",
    "    (\"london\", \"england\", \"france\", \"paris\", \"Geography\"),\n",
    "    (\"tokyo\", \"japan\", \"china\", \"beijing\", \"Geography\"),\n",
    "    \n",
    "    # Comparative\n",
    "    (\"good\", \"better\", \"bad\", \"worse\", \"Comparative\"),\n",
    "    (\"big\", \"bigger\", \"small\", \"smaller\", \"Comparative\"),\n",
    "    \n",
    "    # Verb tenses\n",
    "    (\"walk\", \"walked\", \"talk\", \"talked\", \"Verb Tense\"),\n",
    "    (\"go\", \"went\", \"do\", \"did\", \"Verb Tense\"),\n",
    "    \n",
    "    # Plurals\n",
    "    (\"dog\", \"dogs\", \"cat\", \"cats\", \"Plural\"),\n",
    "    (\"man\", \"men\", \"woman\", \"women\", \"Plural\"),\n",
    "]\n",
    "\n",
    "# Track results\n",
    "success = 0\n",
    "total = 0\n",
    "missing_vocab = 0\n",
    "\n",
    "print()\n",
    "for word_a, word_b, word_c, expected, category in analogies:\n",
    "    result, missing = word_analogy(word_a, word_b, word_c, pytorch_embeddings, pytorch_word2idx, top_n=10)\n",
    "    \n",
    "    print(f\"{word_a} - {word_b} + {word_c} = ? (Expected: {expected})\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"  Missing vocabulary: {missing}\\n\")\n",
    "        missing_vocab += 1\n",
    "        continue\n",
    "    \n",
    "    # Show top 5\n",
    "    top5 = result[:5]\n",
    "    for i, (word, sim) in enumerate(top5, 1):\n",
    "        marker = \"✓\" if word == expected else \" \"\n",
    "        print(f\"  {marker} {i}. {word:15s} ({sim:.3f})\")\n",
    "    \n",
    "    # Check success\n",
    "    predicted_words = [w for w, _ in result]\n",
    "    if expected in predicted_words:\n",
    "        rank = predicted_words.index(expected) + 1\n",
    "        if rank > 5:\n",
    "            print(f\"  (Found at rank {rank})\")\n",
    "        success += 1\n",
    "    \n",
    "    total += 1\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy: {success}/{total} ({success/total*100:.1f}%)\")\n",
    "print(f\"Missing vocabulary: {missing_vocab}\")\n",
    "\n",
    "# Category breakdown\n",
    "print(f\"\\n{'Category':<15s} {'Accuracy'}\")\n",
    "print(\"-\" * 30)\n",
    "categories = {}\n",
    "for word_a, word_b, word_c, expected, category in analogies:\n",
    "    if category not in categories:\n",
    "        categories[category] = {'total': 0, 'success': 0}\n",
    "    categories[category]['total'] += 1\n",
    "    \n",
    "    result, missing = word_analogy(word_a, word_b, word_c, pytorch_embeddings, pytorch_word2idx, top_n=10)\n",
    "    if result:\n",
    "        predicted_words = [w for w, _ in result]\n",
    "        if expected in predicted_words:\n",
    "            categories[category]['success'] += 1\n",
    "\n",
    "for category, stats in sorted(categories.items()):\n",
    "    rate = stats['success'] / stats['total'] * 100\n",
    "    print(f\"{category:<15s} {stats['success']}/{stats['total']} ({rate:.0f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
